{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation types\n",
    "\n",
    "    Holdout(ShuffleSplit)\n",
    "        ngroups = 1\n",
    "        Divides data in 2 parts. Samples do not overlap \n",
    "        Choice if we have enough data or if we are likely to get simmilar scores for different splits\n",
    "        \n",
    "    K-fold(KFold)\n",
    "        ngroups = k\n",
    "        Every sample is used for validation only once.\n",
    "        This method is a good choice when we have a minimum amount of data, and we can get either a sufficiently \n",
    "        big difference in quality, or different optimal parameters between folds.\n",
    "        \n",
    "    Leave-one-out(LeaveOneOut)\n",
    "        ngroups = len(train)\n",
    "        Each time using k-1 object is a train subset and one object left is a test subset. \n",
    "        This method can be helpful if we have too little data and fast enough model to entrain.\n",
    "    \n",
    "    Stratification(StratifiedShuffleSplit) \n",
    "    It is just the way to insure we'll get similar target distribution over different faults.\n",
    "    It is easier to guess that significance of this problem is higher:\n",
    "        first for small data sets, like in this example, \n",
    "        second for unbalanced data sets. For binary classification, that could be, if target average were very \n",
    "        close to 0 or vice versa, very close to 1. \n",
    "        And third, for multiclass classification tasks with huge amount of classes. \n",
    "        \n",
    "        For good classification data sets, stratification split will be quite similar to a simple shuffle split. \n",
    "        That is, to a random split.\n",
    "        \n",
    "        \n",
    "     Time-based splits\n",
    "     \n",
    "         If we make train validation split different from train/test split, \n",
    "         then we are going to create a useless model. \n",
    "         We should, if possible, set up validation to mimic train/test split\n",
    "     \n",
    "         Generate features based on the validation and model type. 'Models indeed differ significantly, \n",
    "         including the fact that most useful features for one model are useless for another.'\n",
    "        \n",
    "          \n",
    "     To be able to find smart ideas for feature generation and to consistently improve our model, \n",
    "     we absolutely want to identify train/test split made by organizers, including the competition, \n",
    "     and reproduce it\n",
    "     \n",
    "     \n",
    "     Splits types:\n",
    "         1. Random, Rowwise (usually then no dependeces between rows)\n",
    "         2. Timewise (features based on target, Moving window)\n",
    "         \n",
    "   <img src=\"files/Images/Moving_window.png\" width=\"800\" height=\"400\">\n",
    "         \n",
    "         3. By id \n",
    "       \n",
    "   <img src=\"files/Images/id_split.png\" width=\"400\" height=\"100\">\n",
    "   \n",
    "         4. Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit #1\n",
    "from sklearn.model_selection import KFold #2\n",
    "from sklearn.model_selection import LeaveOneOut #3\n",
    "from sklearn.model_selection import StratifiedKFold #4.1\n",
    "from sklearn.model_selection import StratifiedShuffleSplit #4.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation problems:\n",
    "    1. Validation stage\n",
    "        1.1 Too little in the data\n",
    "        1.2 Too diversed and inconsistent\n",
    "        \n",
    "        We should do extensive validation:\n",
    "            1. Average scores from different K-Fold splits\n",
    "            2. Tune model on one split, evaluate score on the other\n",
    "            \n",
    "    2. Submision stage\n",
    "        We can observe that:\n",
    "            * LB score is consistenly higher/lower than validation score\n",
    "            * LB score is not correlated with validation score at all\n",
    "            \n",
    "        0. We may already have quire different scores in k-fold\n",
    "        Other reasons:\n",
    "            1. Too little data in public leaderboard\n",
    "            2. incorrect train/test split\n",
    "            3. Train and test data is from different distributions\n",
    "            4. Check if you overfitted\n",
    "        \n",
    "     Leaderboard probing:\n",
    "        The simplest way to solve this particular situation in a competition is to try to figure out\n",
    "        the optimal constant prediction for train and test data. And shift your predictions by the difference.\n",
    "        Get groundtruth public values by submissioms\n",
    "        \n",
    "        \n",
    "     Links:\n",
    "         http://scikit-learn.org/stable/modules/cross_validation.html\n",
    "         http://www.chioka.in/how-to-select-your-final-models-in-a-kaggle-competitio/\n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Leakages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Leaks in time series: \n",
    "        * Splits should be done on time\n",
    "          - In real life we dont have information from future\n",
    "          - In competitions first thinh to look: train/public/private split, is it on time?\n",
    "        * Even when split by time, features may contain information about future.\n",
    "          - User history in CTR tasks\n",
    "          - Weather\n",
    "          \n",
    "    Unexpected informaion:\n",
    "        * Meta data\n",
    "        * Information in IDs (can be generated with hash)\n",
    "        * Row order"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
