{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean encoding (Target encoding)\n",
    "\n",
    "     The general idea of this technique is to add new variables based on some feature to get where we started,. In simplest case, we encode each level of categorical variable with corresponding target mean. \n",
    "     \n",
    "    we can reach better loss with shorter trees. \n",
    "    good indicators for mean encoding:\n",
    "        - The presence of categorical variables with a lot of levels is already a good indicator\n",
    "        \n",
    "    Our model tries to treat all those categories differently and they are also very important for predicting the target. We can help our model via mean encodings.\n",
    "    \n",
    "    \n",
    "<img src=\"files/Images/mean_enc1.png\" width=\"500\" height=\"200\">\n",
    "\n",
    "    Ways to use target variable\n",
    "    \n",
    "<img src=\"files/Images/mean_enc2.png\" width=\"500\" height=\"200\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = X_tr.groupby(col).target.mean()\n",
    "train_new[col + '_mean_target'] = train_new[col].map(means)\n",
    "val_new[col + '_mean_target'] = val_new[col].map(means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(train_new, label=y_tr)\n",
    "dvalid = xgb.DMatrix(val_new, label=y_val)\n",
    "\n",
    "evallist = [(dtrain, 'train'),(dvalid, 'eval')]\n",
    "evals_result3 = {}\n",
    "model = xgb.train(xgb_par, dtrain, 3000, evals = evallist, \n",
    "                  verbose_eval = 30, evals_result = evals_result3,\n",
    "                 early_stopping_rounds = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization\n",
    "\n",
    "    1. CV loop inside training data. (!)\n",
    "        Intuitive and robust method. \n",
    "        Usually decent results with 4-5 folds across different datasets\n",
    "        Need to be careful with extreme situations like LOO\n",
    "<img src=\"files/Images/cvloop.png\" width=\"500\" height=\"200\">\n",
    "\n",
    "    2. Smoothing\n",
    "        Alpha controls the amount of regularization\n",
    "        Only works together with some other regularization method \n",
    "        \n",
    "<img src=\"files/Images/Smoothing.png\" width=\"500\" height=\"200\">\n",
    "\n",
    "    3. Adding random noise\n",
    "        Noise degrades the quality of encoding\n",
    "        How much noise should we add?\n",
    "        Usually used together with LOO\n",
    "        \n",
    "    4. Sorting and calculating expanding mean (!)\n",
    "        Least amount of leakage\n",
    "        No hyper parameters\n",
    "        Irregular encoding quality\n",
    "        Built-in in Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##CV_LOOP\n",
    "y_tr = df_tr['target'].values()\n",
    "skf = StratifiedKFold(y_tr, 5, shuffle = True, random_state = 123)\n",
    "\n",
    "for tr_ind, val_ind in skf:\n",
    "    X_tr, X_val = df_tr.iloc[tr_ind], df_tr.iloc[val_ind]\n",
    "    for col in cols: #iterate though the columns we want to encode\n",
    "        means = X_val[col].map(X_tr.groupby(col).target.mean())\n",
    "        X_val[col + '_mean_target'] = means\n",
    "    train_new.iloc[val_ind] = X_val\n",
    "prior = df_tr['target'].mean() #global mean\n",
    "train_new.fillna(prior, inplace = True) #fill na with global means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Calculating expanding mean\n",
    "cumsum = df_tr.groupby(col)['target'].cumsum() - df_tr['target']\n",
    "cumcnt = df_tr.groupby(col).cumscount()\n",
    "train_new[col + '_mean_target'] = cumsum/cumcnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extensions and generalizations\n",
    "\n",
    "    Regression and multiclass:\n",
    "        More statistics for regression tasks. Percentiles, std, distribution bins\n",
    "        Introducing new information for one vs all classifiers in multi class tasks\n",
    "    Many to many relations:\n",
    "        Cross product of entities (Long representation)\n",
    "        Statistics from vectors\n",
    "    Times series:\n",
    "        Time structure allows us to make a lot of complicated features\n",
    "        Rolling statistics of target variable\n",
    "     Interections and numerical features\n",
    "         Analysing fitted model\n",
    "         Binnin numeric and selecting interactions\n",
    "        \n",
    "     Correct validation reminder\n",
    "     \n",
    "<img src=\"files/Images/valrem.png\" width=\"500\" height=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main advantages:\n",
    "    * Compact transformation of categorical variables\n",
    "    * Powerful basis for feature engineering\n",
    "\n",
    "Disadvantages:\n",
    "    * Need careful validation, there are a lot of ways to overfit\n",
    "    * Significant improvements only on specific datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
