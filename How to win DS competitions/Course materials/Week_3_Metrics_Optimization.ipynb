{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ## If your model is scored with some metric, you get best results by optimizing exactly this metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploritary metric analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Regression\n",
    "     * MSE, RMSE, R-squared\n",
    "       MSE, Best constant for base model - target mean value\n",
    "\n",
    "<img src=\"files/Images/MSE.png\" width=\"400\" height=\"100\">\n",
    "\n",
    "       RMSE - skale for the error is the same as target values. We can optimize MSE instead of RMSE. \n",
    "       \n",
    "<img src=\"files/Images/RMSE.png\" width=\"400\" height=\"100\">\n",
    "\n",
    "        R-squered used to measure relults in range. To optimize R-squared we can optimize MSE.\n",
    "        \n",
    "<img src=\"files/Images/rsquered.png\" width=\"400\" height=\"100\">\n",
    "        \n",
    "     * MAE\n",
    "     Not that sensetive to outliets as MSE. Used often in Finance. Best constant for base model - Median. More robust than MSE.\n",
    "     \n",
    "<img src=\"files/Images/MAE.png\" width=\"400\" height=\"100\">\n",
    "\n",
    "    Non-deferrable at zero\n",
    "\n",
    "\n",
    "<img src=\"files/Images/MAE_grad.png\" width=\"400\" height=\"100\">\n",
    "       \n",
    "       \n",
    "   #### total on mse/mae\n",
    "    Do you Have outliers in the data?\n",
    "        Use MAE\n",
    "    Are you sure they are outliers?\n",
    "        Use MAE\n",
    "    Or they are just unexpected values we should still care about?\n",
    "        Use MSE\n",
    "        \n",
    "     * (R)MSPE, MAPE\n",
    "     \n",
    "     these metrics are weighted according to target values\n",
    "     \n",
    "<img src=\"files/Images/all_reg.png\" width=\"400\" height=\"100\">\n",
    "<img src=\"files/Images/MSPE.png\" width=\"400\" height=\"100\">\n",
    "<img src=\"files/Images/MAPE.png\" width=\"400\" height=\"100\">\n",
    "\n",
    "     * (R)MSLE\n",
    "     \n",
    "     Cares about relative errors(as MSPE and MAPE) more than about absolute ones\n",
    "<img src=\"files/Images/MSLE.png\" width=\"400\" height=\"100\">   \n",
    "\n",
    "     From the perspective of RMSLE, it is always better to predict more than the same amount less than target.\n",
    "<img src=\"files/Images/MSLE2.png\" width=\"400\" height=\"100\">  \n",
    "     \n",
    " \n",
    " \n",
    "#### So:\n",
    "    MSE is quite biased towards the huge value from our dataset, while MAE is much less biased. MSPE and MAPE are biased towards smaller targets because they assign higher weight to the object with small targets. And RMSLE is frequently considered as better metrics than MAPE, since it is less biased towards small targets, yet works with relative errors. \n",
    "\n",
    "<img src=\"files/Images/compare.png\" width=\"400\" height=\"100\">  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Classification\n",
    "      * Accuracy, LogLoss, AUC\n",
    "      \n",
    "      \n",
    "      \n",
    "      \n",
    "      * Cohen's(Quadratic weighted) Kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
