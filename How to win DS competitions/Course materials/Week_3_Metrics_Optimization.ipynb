{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ## If your model is scored with some metric, you get best results by optimizing exactly this metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploritary metric analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Regression\n",
    "     * MSE, RMSE, R-squared\n",
    "       MSE, Best constant for base model - target mean value\n",
    "\n",
    "<img src=\"files/Images/MSE.png\" width=\"400\" height=\"100\">\n",
    "\n",
    "       RMSE - skale for the error is the same as target values. We can optimize MSE instead of RMSE. \n",
    "       \n",
    "<img src=\"files/Images/RMSE.png\" width=\"400\" height=\"100\">\n",
    "\n",
    "        R-squered used to measure relults in range. To optimize R-squared we can optimize MSE.\n",
    "        \n",
    "<img src=\"files/Images/rsquered.png\" width=\"400\" height=\"100\">\n",
    "        \n",
    "     * MAE\n",
    "     Not that sensetive to outliets as MSE. Used often in Finance. Best constant for base model - Median. More robust than MSE.\n",
    "     \n",
    "<img src=\"files/Images/MAE.png\" width=\"400\" height=\"100\">\n",
    "\n",
    "    Non-deferrable at zero\n",
    "\n",
    "\n",
    "<img src=\"files/Images/MAE_grad.png\" width=\"400\" height=\"100\">\n",
    "       \n",
    "       \n",
    "   #### total on mse/mae\n",
    "    Do you Have outliers in the data?\n",
    "        Use MAE\n",
    "    Are you sure they are outliers?\n",
    "        Use MAE\n",
    "    Or they are just unexpected values we should still care about?\n",
    "        Use MSE\n",
    "        \n",
    "     * (R)MSPE, MAPE\n",
    "     \n",
    "     these metrics are weighted according to target values\n",
    "     \n",
    "<img src=\"files/Images/All_reg.png\" width=\"400\" height=\"100\">\n",
    "<img src=\"files/Images/MSPE.png\" width=\"400\" height=\"100\">\n",
    "<img src=\"files/Images/MAPE.png\" width=\"400\" height=\"100\">\n",
    "\n",
    "     * (R)MSLE\n",
    "     \n",
    "     Cares about relative errors(as MSPE and MAPE) more than about absolute ones\n",
    "<img src=\"files/Images/MSLE.png\" width=\"400\" height=\"100\">   \n",
    "\n",
    "     From the perspective of RMSLE, it is always better to predict more than the same amount less than target.\n",
    "<img src=\"files/Images/MSLE2.png\" width=\"400\" height=\"100\">  \n",
    "     \n",
    " \n",
    " \n",
    "#### So:\n",
    "    MSE is quite biased towards the huge value from our dataset, while MAE is much less biased. MSPE and MAPE are biased towards smaller targets because they assign higher weight to the object with small targets. And RMSLE is frequently considered as better metrics than MAPE, since it is less biased towards small targets, yet works with relative errors. \n",
    "\n",
    "<img src=\"files/Images/compare.png\" width=\"400\" height=\"100\">  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Classification\n",
    "      * Accuracy, LogLoss, AUC\n",
    "      \n",
    "      Accuracy\n",
    "<img src=\"files/Images/accuracy.png\" width=\"400\" height=\"100\"> \n",
    "\n",
    "      LogLoss\n",
    "<img src=\"files/Images/logloss.png\" width=\"400\" height=\"100\">  \n",
    "\n",
    "      Best constant for logloss - set aj to freaquency of i-th class \n",
    "<img src=\"files/Images/loglossacc.png\" width=\"400\" height=\"100\">  \n",
    "\n",
    "      AUC. \n",
    "      Best constant: All constants give same score\n",
    "      Random predictions lead to AUC = 0.5\n",
    "<img src=\"files/Images/rocauc.png\" width=\"400\" height=\"100\">  \n",
    "\n",
    "      Examples of building area under curve\n",
    "<img src=\"files/Images/rocex.png\" width=\"400\" height=\"100\">  \n",
    "<img src=\"files/Images/rocex2.png\" width=\"400\" height=\"100\">  \n",
    "      \n",
    "      \n",
    "      \n",
    "      * Cohen's(Quadratic weighted) Kappa\n",
    "      \n",
    "<img src=\"files/Images/kappa.png\" width=\"400\" height=\"100\">  \n",
    "<img src=\"files/Images/kappa1.png\" width=\"400\" height=\"100\">  \n",
    "<img src=\"files/Images/kappa2.png\" width=\"400\" height=\"100\">  \n",
    "<img src=\"files/Images/kappa3.png\" width=\"400\" height=\"100\">  \n",
    "<img src=\"files/Images/kappa4.png\" width=\"400\" height=\"100\">  \n",
    "\n",
    "\n",
    "   #### So:\n",
    "        The accuracy is an essential metric for classification. But a simple model that predicts always the same value can possibly have a very high accuracy that makes it hard to interpret this metric. The score also depends on the threshold we choose to convert soft predictions to hard labels. Logloss is another metric, as opposed to accuracy it depends on soft predictions rather than on hard labels. And it forces the model to predict probabilities of an object to belong to each class. AUC, area under receiver operating curve, doesn't depend on the absolute values predicted by the classifier, but only considers the ordering of the object. It also implicitly tries all the thresholds to converge soft predictions to hard labels, and thus removes the dependence of the score on the threshold. Finally, Cohen's Kappa fixes the baseline for accuracy score to be zero. In spirit it is very similar to how R-squared beta scales MSE value to be easier explained. If instead of accuracy we used weighted accuracy, we would get weighted kappa. Weighted kappa with quadratic weights is called quadratic weighted kappa and commonly used on Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) General approach\n",
    "    Loss and Metric\n",
    "    Target metric is what we want to optimize\n",
    "    Optimization metric is what model optimizes\n",
    "    \n",
    "    Approaches for target metric optimization\n",
    "    \n",
    "       * Just run the right model\n",
    "           - Mse, Logloss\n",
    "       * Preprocess train and optimize another metric\n",
    "           - MSPE, MAPE, RMSLE\n",
    "       * Optimize another metric, postprocess predictions\n",
    "           - Accuracy, Kappa\n",
    "       * Write custom loss function\n",
    "           - Any\n",
    "           \n",
    "       Early stopping\n",
    "          \n",
    "<img src=\"files/Images/es.png\" width=\"400\" height=\"100\">  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## How to write custom loss functions?\n",
    "def logregobj(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    preds = 1.0/(1.0 + np.exp(-preds))\n",
    "    grad = preds - labels\n",
    "    hess = preds * (1.0 - preds)\n",
    "    return grad, hess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Regression metrics \n",
    "    * RMSE, MSE, R-squared\n",
    "    \n",
    "    Libraries what support MSE loss function\n",
    "    \n",
    "<img src=\"files/Images/mse_op.png\" width=\"400\" height=\"100\">  \n",
    "\n",
    "    Libraries what support MAE loss function. MAE dont have second derivitive\n",
    "<img src=\"files/Images/mae_op.png\" width=\"400\" height=\"100\">  \n",
    "\n",
    "    Ways how to make MAE smooth\n",
    "<img src=\"files/Images/mae_op2.png\" width=\"400\" height=\"100\">  \n",
    "\n",
    "    \n",
    "    \n",
    "    * MSPE and MAPE\n",
    "    \n",
    "    MSPE(MAPE) as weighted MSE(MAE)\n",
    "<img src=\"files/Images/weights.png\" width=\"400\" height=\"100\">   \n",
    "    \n",
    "    Approaches:\n",
    "<img src=\"files/Images/mspe_op.png\" width=\"400\" height=\"100\"> \n",
    "\n",
    "     \n",
    "    * RMSLE\n",
    "    \n",
    "    Approaches:\n",
    "<img src=\"files/Images/rmsle_op.png\" width=\"400\" height=\"100\">   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Classification metrics\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
