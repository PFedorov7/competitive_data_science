{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ## If your model is scored with some metric, you get best results by optimizing exactly this metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploritary metric analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Regression\n",
    "     * MSE, RMSE, R-squared\n",
    "       MSE, Best constant for base model - target mean value\n",
    "\n",
    "<img src=\"files/Images/MSE.png\" width=\"400\" height=\"100\">\n",
    "\n",
    "       RMSE - skale for the error is the same as target values. We can optimize MSE instead of RMSE. \n",
    "       \n",
    "<img src=\"files/Images/RMSE.png\" width=\"400\" height=\"100\">\n",
    "\n",
    "        R-squered used to measure relults in range. To optimize R-squared we can optimize MSE.\n",
    "        \n",
    "<img src=\"files/Images/rsquered.png\" width=\"400\" height=\"100\">\n",
    "        \n",
    "     * MAE\n",
    "     Not that sensetive to outliets as MSE. Used often in Finance. Best constant for base model - Median. More robust than MSE.\n",
    "     \n",
    "<img src=\"files/Images/MAE.png\" width=\"400\" height=\"100\">\n",
    "\n",
    "    Non-deferrable at zero\n",
    "\n",
    "\n",
    "<img src=\"files/Images/MAE_grad.png\" width=\"400\" height=\"100\">\n",
    "       \n",
    "       \n",
    "   #### total on mse/mae\n",
    "    Do you Have outliers in the data?\n",
    "        Use MAE\n",
    "    Are you sure they are outliers?\n",
    "        Use MAE\n",
    "    Or they are just unexpected values we should still care about?\n",
    "        Use MSE\n",
    "        \n",
    "     * (R)MSPE, MAPE\n",
    "     \n",
    "     these metrics are weighted according to target values\n",
    "     \n",
    "<img src=\"files/Images/All_reg.png\" width=\"400\" height=\"100\">\n",
    "<img src=\"files/Images/MSPE.png\" width=\"400\" height=\"100\">\n",
    "<img src=\"files/Images/MAPE.png\" width=\"400\" height=\"100\">\n",
    "\n",
    "     * (R)MSLE\n",
    "     \n",
    "     Cares about relative errors(as MSPE and MAPE) more than about absolute ones\n",
    "<img src=\"files/Images/MSLE.png\" width=\"400\" height=\"100\">   \n",
    "\n",
    "     From the perspective of RMSLE, it is always better to predict more than the same amount less than target.\n",
    "<img src=\"files/Images/MSLE2.png\" width=\"400\" height=\"100\">  \n",
    "     \n",
    " \n",
    " \n",
    "#### So:\n",
    "    MSE is quite biased towards the huge value from our dataset, while MAE is much less biased. MSPE and MAPE are biased towards smaller targets because they assign higher weight to the object with small targets. And RMSLE is frequently considered as better metrics than MAPE, since it is less biased towards small targets, yet works with relative errors. \n",
    "\n",
    "<img src=\"files/Images/compare.png\" width=\"400\" height=\"100\">  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Classification\n",
    "      * Accuracy, LogLoss, AUC\n",
    "      \n",
    "      Accuracy\n",
    "<img src=\"files/Images/accuracy.png\" width=\"400\" height=\"100\"> \n",
    "\n",
    "      LogLoss\n",
    "<img src=\"files/Images/logloss.png\" width=\"400\" height=\"100\">  \n",
    "\n",
    "      Best constant for logloss - set aj to freaquency of i-th class \n",
    "<img src=\"files/Images/loglossacc.png\" width=\"400\" height=\"100\">  \n",
    "\n",
    "      AUC. \n",
    "      Best constant: All constants give same score\n",
    "      Random predictions lead to AUC = 0.5\n",
    "<img src=\"files/Images/rocauc.png\" width=\"400\" height=\"100\">  \n",
    "\n",
    "      Examples of building area under curve\n",
    "<img src=\"files/Images/rocex.png\" width=\"400\" height=\"100\">  \n",
    "<img src=\"files/Images/rocex2.png\" width=\"400\" height=\"100\">  \n",
    "      \n",
    "      \n",
    "      \n",
    "      * Cohen's(Quadratic weighted) Kappa\n",
    "      \n",
    "<img src=\"files/Images/kappa.png\" width=\"400\" height=\"100\">  \n",
    "<img src=\"files/Images/kappa1.png\" width=\"400\" height=\"100\">  \n",
    "<img src=\"files/Images/kappa2.png\" width=\"400\" height=\"100\">  \n",
    "<img src=\"files/Images/kappa3.png\" width=\"400\" height=\"100\">  \n",
    "<img src=\"files/Images/kappa4.png\" width=\"400\" height=\"100\">  \n",
    "\n",
    "\n",
    "   #### So:\n",
    "        The accuracy is an essential metric for classification. But a simple model that predicts always the same value can possibly have a very high accuracy that makes it hard to interpret this metric. The score also depends on the threshold we choose to convert soft predictions to hard labels. Logloss is another metric, as opposed to accuracy it depends on soft predictions rather than on hard labels. And it forces the model to predict probabilities of an object to belong to each class. AUC, area under receiver operating curve, doesn't depend on the absolute values predicted by the classifier, but only considers the ordering of the object. It also implicitly tries all the thresholds to converge soft predictions to hard labels, and thus removes the dependence of the score on the threshold. Finally, Cohen's Kappa fixes the baseline for accuracy score to be zero. In spirit it is very similar to how R-squared beta scales MSE value to be easier explained. If instead of accuracy we used weighted accuracy, we would get weighted kappa. Weighted kappa with quadratic weights is called quadratic weighted kappa and commonly used on Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) General approach\n",
    "    Loss and Metric\n",
    "    Target metric is what we want to optimize\n",
    "    Optimization metric is what model optimizes\n",
    "    \n",
    "    Approaches for target metric optimization\n",
    "    \n",
    "       * Just run the right model\n",
    "           - Mse, Logloss\n",
    "       * Preprocess train and optimize another metric\n",
    "           - MSPE, MAPE, RMSLE\n",
    "       * Optimize another metric, postprocess predictions\n",
    "           - Accuracy, Kappa\n",
    "       * Write custom loss function\n",
    "           - Any\n",
    "           \n",
    "       Early stopping\n",
    "          \n",
    "<img src=\"files/Images/es.png\" width=\"400\" height=\"100\">  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## How to write custom loss functions?\n",
    "def logregobj(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    preds = 1.0/(1.0 + np.exp(-preds))\n",
    "    grad = preds - labels\n",
    "    hess = preds * (1.0 - preds)\n",
    "    return grad, hess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Regression metrics \n",
    "    * RMSE, MSE, R-squared\n",
    "    \n",
    "    Libraries what support MSE loss function\n",
    "    \n",
    "<img src=\"files/Images/mse_op.png\" width=\"400\" height=\"100\">  \n",
    "\n",
    "    Libraries what support MAE loss function. MAE dont have second derivitive\n",
    "<img src=\"files/Images/mae_op.png\" width=\"400\" height=\"100\">  \n",
    "\n",
    "    Ways how to make MAE smooth\n",
    "<img src=\"files/Images/mae_op2.png\" width=\"400\" height=\"100\">  \n",
    "\n",
    "    \n",
    "    \n",
    "    * MSPE and MAPE\n",
    "    \n",
    "    MSPE(MAPE) as weighted MSE(MAE)\n",
    "<img src=\"files/Images/weights.png\" width=\"400\" height=\"100\">   \n",
    "    \n",
    "    Approaches:\n",
    "<img src=\"files/Images/mspe_op.png\" width=\"400\" height=\"100\"> \n",
    "\n",
    "     \n",
    "    * RMSLE\n",
    "    \n",
    "    Approaches:\n",
    "<img src=\"files/Images/rmsle_op.png\" width=\"400\" height=\"100\">   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Classification metrics\n",
    "    \n",
    "    * Logloss\n",
    "    Probability calibration:\n",
    "        - Platt scaling\n",
    "          Just fit Logistic Regression to your predictions(like in stacking)\n",
    "        - Isotonic regression\n",
    "          Just fit Isotonic Regression to your predictions(like in stacking)\n",
    "        - Stacking \n",
    "          Just fit XGBoost ot neural net to your predictions\n",
    "    \n",
    "<img src=\"files/Images/loglossop.png\" width=\"400\" height=\"100\">  \n",
    "    \n",
    "    \n",
    "     that model on top will use logloss as its optimization loss. So it will be optimizing indirectly, and its predictions will be calibrated.\n",
    "     \n",
    "     \n",
    "    * Accuracy\n",
    "    If it is a binary classification task, fit any metric, and tune with the binarization threshold. For multi-class tasks, fit any metric and tune parameters comparing the models by their accuracy score, not by the metric that the models were really optimizing.\n",
    "    \n",
    "    The problem is that, this loss has zero almost everywhere gradient, with respect to the predictions. And most learning algorithms require a nonzero gradient to fit, otherwise it's not clear how we need to change the predictions such that loss is decreased.\n",
    "    \n",
    "    And so people came up with proxy losses that are upper bounds for these zero-one loss. So if you perfectly fit the proxy loss, the accuracy will be perfect too.\n",
    "    \n",
    "     We can tune the threshold we apply, we can do it with a simple grid search implemented with a for loop. Well, it means that we can basically fit any sufficiently powerful model. It will not matter much what loss exactly, say, hinge or log loss the model will optimize. All we want from our model's predictions is the existence of a good threshold that will separate the classes.\n",
    "     \n",
    "     \n",
    "     * AUC\n",
    "     The loss function of AUC has zero gradients almost everywhere, exactly as accuracy loss.\n",
    "     instead of using pointwise loss, we should use pairwise loss. A pairwise loss takes predictions and labels for a pair of objects and computes their loss. Ideally, the loss would be zero when the ordering is correct, and greater than zero when the ordering is not incorrect. \n",
    "     \n",
    "     \n",
    "<img src=\"files/Images/pairwiseloss.png\" width=\"400\" height=\"100\"> \n",
    "<img src=\"files/Images/pairwiseloss1.png\" width=\"400\" height=\"100\"> \n",
    "<img src=\"files/Images/rocaucop.png\" width=\"400\" height=\"100\"> \n",
    "\n",
    "    * Quadratic weighted Kappa\n",
    "        1) Optimize MSE and find right thresholds(Simple)\n",
    "        2) Custom smooth loss for GBDT or neural nets(Harder)\n",
    "        \n",
    "<img src=\"files/Images/kappaop.png\" width=\"400\" height=\"100\"> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional Links\n",
    "\n",
    "    Classification:\n",
    "* http://queirozf.com/entries/evaluation-metrics-for-classification-quick-examples-references\n",
    "* https://www.garysieling.com/blog/sklearn-gini-vs-entropy-criteria\n",
    "* http://www.navan.name/roc/\n",
    "    \n",
    "    Ranking:\n",
    "* http://icml.cc/2015/wp-content/uploads/2015/06/icml_ranking.pdf\n",
    "* https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/MSR-TR-2010-82.pdf\n",
    "* https://sourceforge.net/p/lemur/wiki/RankLib/\n",
    "* https://wellecks.wordpress.com/2015/01/15/learning-to-rank-overview\n",
    "\n",
    "    Clustering:\n",
    "* http://nlp.uned.es/docs/amigo2007a.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
