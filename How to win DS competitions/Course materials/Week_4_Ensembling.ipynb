{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avereging ensemble methods\n",
    "\n",
    "    Simple averaging: (model1 + model2) / 2\n",
    "    The average error is smaller. However, the model doesn't do better as an individual models for the areas where the models were doing really well, nevertheless, it does better on average.\n",
    "    \n",
    "    Weighted averaging: model1 * 0.7 + model2 * 0.3\n",
    "    If the models have quite similar predictive powen, than their square is no better and that makes sense and it doesn't make sense to rely more in one.\n",
    "    \n",
    "    Condition averaging: Prediction of model1 if condition1 else prediction of model2\n",
    "    There are ensemble methods that are very good at finding these relationships of two or more predictions in respect to the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging\n",
    "\n",
    "    Means avereging slightly different versions of the same model to improve accuracy - Decision tree\n",
    "    There are two main sources of errors in modeling:\n",
    "        1. Errors due to Bias (underfitting)\n",
    "        2. Errors due to Varience (overfitting)\n",
    "    Bagging parameteres:\n",
    "        1. Changing the seed\n",
    "        2. Row(Sub) sampling ot Bootstrapping\n",
    "        3. Shuffling\n",
    "        4. Column(Sub) sampling\n",
    "        5. Model-specific parameteres\n",
    "        6. Number of models (or bags) usually more than 10\n",
    "        7. (Optionally) parallelism\n",
    "    Models are independent of each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier, BaggingRegressor\n",
    "\n",
    "model = RandomForestRegressor()\n",
    "bags = 10\n",
    "seed = 1\n",
    "\n",
    "bagged_prediction = np.zeroz(test.shape[0])\n",
    "for n in range(0, bags):\n",
    "    model.set_params(random_state = seed + n)\n",
    "    model.fit(train, y)\n",
    "    preds = model.predict(test)\n",
    "    bagged_prediction += preds\n",
    "bagged_prediction /= bags    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting\n",
    "\n",
    "    A form of weighted avareging of models where each model os built sequentially via taking into account the past model performance\n",
    "    Main boosting types:\n",
    "        * Weight based\n",
    "        * Residual based\n",
    "        \n",
    "     1) Weight based Boosting\n",
    "     \n",
    "         Step 1. Make predictions -> Calculate absolute errors -> Create weights based on it \n",
    "         \n",
    "<img src=\"files/Images/boost1.png\" width=\"400\" height=\"100\"> \n",
    "\n",
    "         Step 2. Add next model into ensemble and take into account the weights from the previous model. Rows with bigger weight would get more attention while building new predictions\n",
    "         \n",
    "<img src=\"files/Images/boost2.png\" width=\"400\" height=\"100\"> \n",
    "\n",
    "    \n",
    "         Parameters:\n",
    "             1. Learning rate (or shrinkage or eta)\n",
    "             predictionN = pred0*eta + pred1*eta + ... + predN * eta\n",
    "             2. Number of estimators(The relationshop between number of estimators and learning rate is close to linear) To incr\n",
    "                The logic: Take 100 estimators at start -> find best learning rate(0.1). Than to increse estimators number twice(200) we should divide learning rate by 2 (0,05)\n",
    "             3. Input model - can be anything that accepts weights\n",
    "             4. Sub boosting type:\n",
    "                 * Adaboost - Sklearn(Python)\n",
    "                 * Logitboost - Weka(Java)\n",
    "                 \n",
    "                 \n",
    "      2) Residual based Boosting\n",
    "          Extremly successful \n",
    "          \n",
    "          Step 1. Ð¡alculate the error of these predictions but this time, not in absolute terms because we're interested about the direction of the error.\n",
    "<img src=\"files/Images/boost3.png\" width=\"400\" height=\"100\"> \n",
    "                  \n",
    "          Step 2. Adding new y variable so the error now becomes the new target variable and we use the same features in order to predict this error. To predict Rownum = 1 we would say: Final prediction = 0.75 + 0.20 = 0.95\n",
    "<img src=\"files/Images/boost4.png\" width=\"400\" height=\"100\"> \n",
    "\n",
    "         Parameters:\n",
    "             1. Learning rate (or shrinkage or eta)\n",
    "             predictionN = pred0 + pred1*eta + ... + predN * eta\n",
    "             2. Number of estimators\n",
    "             3. Row sub() sampling\n",
    "             4. Column sub() sampling\n",
    "             5. Input method - best works with trees\n",
    "             6. Sub boosting type:\n",
    "                 * Fully gradient based\n",
    "                 * Dart\n",
    "          Implementation: \n",
    "              * XGBoost\n",
    "              * LightGBM\n",
    "              * H2O's GBM\n",
    "              * Catboost\n",
    "              * Sklearn's GBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking\n",
    "\n",
    "    Means making predictions of a number of models in a hold-out set and then using a different (Meta) model to train on these predictions\n",
    "    \n",
    "    Methodology:\n",
    "        1. Splitting the train set into two disjoint sets\n",
    "        2. Train several base learners on the first part\n",
    "        3. Make predictions with the base learners on the second(validation) part\n",
    "        4. Using the predictions from (3) as the input a higher level learner\n",
    "        \n",
    "<img src=\"files/Images/stack.png\" width=\"400\" height=\"100\"> \n",
    "<img src=\"files/Images/stack2.png\" width=\"400\" height=\"100\"> \n",
    "    \n",
    "    Thinks to be mindful of:\n",
    "        * With time-sensetive data - respect time\n",
    "        * Diversity as important as performance\n",
    "        * Diversity may come from:\n",
    "            - Different algorighms\n",
    "            - Different input features\n",
    "        * Performance plateuing after N models\n",
    "        * Meta model is normally modest\n",
    "        \n",
    "    It will find when a model is good, and when a model is actually bad or fairly weak. So you don't need to worry too much to make all the models really strong, stacking can actually extract the juice from each prediction. Therefore, what you really need to focus is, am I making a model that brings some information, even though it is generally weak? Weak models can bring in new information that the meta model could leverage.\n",
    "     For example, in one data set you may treat categorical features as one whole encoding. In another, you may just use label in coding, and the result will probably produce a model that is very different.\n",
    "     \n",
    "    Therefore, it is quite often that the meta model is generally simpler. So if I was to express this in a random forest context, it will have lower depth than what was the best one you found in your base models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training, valid, ytraining, yvalid = train_test_split(train, y, test_size = 0.5)\n",
    "\n",
    "model1 = RandomForestRegressor()\n",
    "model2 = LinearRegression()\n",
    "\n",
    "model1.fit(training, ytraining)\n",
    "model2.fit(training, ytraining)\n",
    "\n",
    "preds1 = model1.predict(valid)\n",
    "preds2 = model2.predict(valid)\n",
    "\n",
    "test_preds1 = model1.predict(test)\n",
    "test_preds2 = model2.predict(test)\n",
    "\n",
    "stacking_predictions = np.column_stack((preds1, preds2))\n",
    "stacked_test_predictions = np.column_stack((test_preds1, test_preds2))\n",
    "\n",
    "meta_model = LinearRegression()\n",
    "meta_model.fit(stacking_predictions, yvalid)\n",
    "final_predictions = meta_model.predict(stacked_test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StackNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
