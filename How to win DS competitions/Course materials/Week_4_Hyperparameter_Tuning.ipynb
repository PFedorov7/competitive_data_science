{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparametets in general\n",
    "    \n",
    "    1. Select the most influential parameters\n",
    "    2. Understand, how exactly they influence the training\n",
    "    3. Tune them!\n",
    "        a. Manually\n",
    "        b. Automatically\n",
    "        \n",
    "    Optimization software:\n",
    "        - Hyperopt\n",
    "        - Scikit-optimize\n",
    "        - Spearmint\n",
    "        - GPyOpt\n",
    "        - RoBO\n",
    "        - SMAC3\n",
    "        \n",
    "     Example\n",
    "  \n",
    "<img src=\"files/Images/hyperopt.png\" width=\"400\" height=\"100\"> \n",
    "\n",
    "    \n",
    "    Color-coding legend:\n",
    "\n",
    "   1. <font color='green'>Underfitting</font>\n",
    "   \n",
    "   2. <font color='black'>Good fit and generalisation</font>\n",
    "   \n",
    "   3. <font color='red'>Overfitting</font>\n",
    "   \n",
    "   \n",
    "   * A parameter in red\n",
    "       - Increasing it impedes fitting\n",
    "       - Increase it to reduce overfitting\n",
    "       - Decrease to allow model fit easier\n",
    "       \n",
    "   * A parameter in green\n",
    "       - Increasing it leads to a better fit(overfit) on train set\n",
    "       - Increase it, if model underfits\n",
    "       - Decrease if overfits "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tree-based models\n",
    "    \n",
    "    Some implementations\n",
    "<img src=\"files/Images/trees.png\" width=\"400\" height=\"100\"> \n",
    "\n",
    "    Compare GBDT\n",
    "<img src=\"files/Images/gbdtcomp.png\" width=\"400\" height=\"100\"> \n",
    "\n",
    "    Extra Trees\n",
    "<img src=\"files/Images/extratrees.png\" width=\"400\" height=\"100\"> \n",
    "\n",
    "     If you increase the depth and can not get the model to overfit, that is, the model is becoming better and better on the validation set as you increase the depth. It can be a sign that there are a lot of important interactions to extract from the data. So it's better to stop tuning and try to generate some features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NeuralNets\n",
    "    \n",
    "    Hyperparameters:\n",
    "<img src=\"files/Images/nnh.png\" width=\"400\" height=\"100\"> \n",
    "\n",
    "    You can override dropout probability and a place where you insert the dropout layer. Usually people add the dropout layer closer to the end of the network, but it's okay to add some dropout to every layer, it also works.\n",
    "    Dropout helps network to find features that really matters, and what never worked for me is to have dropout as the very first layer, immediately after data layer.\n",
    "    \n",
    "    Static dropconnect:\n",
    "<img src=\"files/Images/staticdrop.png\" width=\"400\" height=\"100\">     \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear models\n",
    "\n",
    "    Implementations:\n",
    "   \n",
    "<img src=\"files/Images/linimp.png\" width=\"400\" height=\"100\">   \n",
    "\n",
    "    hyperparameters:\n",
    "    \n",
    "<img src=\"files/Images/hyperlin.png\" width=\"400\" height=\"100\">  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tips\n",
    "\n",
    "<img src=\"files/Images/tips.png\">  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional links:\n",
    "\n",
    "   1. Tuning the hyper-parameters of an estimator (sklearn):\n",
    "    http://scikit-learn.org/stable/modules/grid_search.html\n",
    "   2. Optimizing hyperparameters with hyperopt:\n",
    "    http://fastml.com/optimizing-hyperparams-with-hyperopt/\n",
    "   3. Complete Guide to Parameter Tuning in Gradient Boosting (GBM) in Python\n",
    "    https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#factorization machines\n",
    "#early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
