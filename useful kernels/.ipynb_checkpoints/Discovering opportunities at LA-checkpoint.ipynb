{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Science for Good: City of Los Angeles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://media.giphy.com/media/3XAU2dw8fjghZmsRZd/giphy.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is to convert a folder full of plain-text job postings into a single structured CSV file and then to use this data to:\n",
    "1.  identify language that can negatively bias the pool of applicants; \n",
    "2.  improve the diversity and quality of the applicant pool; and/or \n",
    "3.  make it easier to determine which promotions are available to employees in each job class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "MagickWand shared library not found.\nYou probably had not installed ImageMagick library.\nTry to install:\n  brew install freetype imagemagick",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/wand/api.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     \u001b[0mlibraries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_library\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mOSError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/wand/api.py\u001b[0m in \u001b[0;36mload_library\u001b[0;34m()\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlibwand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlibmagick\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cannot find library; tried paths: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtried_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: cannot find library; tried paths: []",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-668147bdf40c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mwand\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mImg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mImg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'CityofLA/Additional data/PDFs/2017/july 2017/July 21/ARTS ASSOCIATE 2454 072117 REV 072817.pdf'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresolution\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/wand/image.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0massertions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlibc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlibmagick\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlibrary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcolor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mColor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/wand/assertions.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;31m# Lazy load recursive import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcolor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mColor\u001b[0m  \u001b[0;31m# noqa: E402\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/wand/color.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumbers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlibrary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcdefs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstructures\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMagickPixelPacket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPixelInfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/wand/api.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    161\u001b[0m     raise ImportError('MagickWand shared library not found.\\n'\n\u001b[1;32m    162\u001b[0m                       \u001b[0;34m'You probably had not installed ImageMagick library.\\n'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m                       'Try to install:\\n  ' + msg)\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;31m#: (:class:`ctypes.CDLL`) The MagickWand library.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: MagickWand shared library not found.\nYou probably had not installed ImageMagick library.\nTry to install:\n  brew install freetype imagemagick"
     ]
    }
   ],
   "source": [
    "from wand.image import Image as Img\n",
    "Img(filename='CityofLA/Additional data/PDFs/2017/july 2017/July 21/ARTS ASSOCIATE 2454 072117 REV 072817.pdf', resolution=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is in this kernel?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. As my first step toward solving the problem,i will focus on reading the job bulletin files and extracting \n",
    "    the required data from it.\n",
    "2. Exploratory data analysis\n",
    "3. Suggestions to improve diversity.\n",
    "- NOTE : kernel under construction,more to come !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from collections  import Counter\n",
    "from nltk import word_tokenize\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import calendar\n",
    "from wordcloud import WordCloud ,STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "print(os.listdir(\"../input\"))\n",
    "from gensim.models import word2vec\n",
    "from sklearn.manifold import TSNE\n",
    "from nltk import pos_tag\n",
    "from nltk.help import upenn_tagset\n",
    "import gensim\n",
    "import matplotlib.colors as mcolors\n",
    "from nltk import jaccard_distance\n",
    "from nltk import ngrams\n",
    "#import textstat\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### checking all subdirectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "files=[dir for dir in os.walk('../input/cityofla')]\n",
    "for file in files:\n",
    "    print(os.listdir(file[0]))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- job bulletins :This directory contains the job bulletins in text format.\n",
    "- additional data :This directory contains additional data in pdf and csv format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bulletins=os.listdir(\"../input/cityofla/CityofLA/Job Bulletins/\")\n",
    "additional=os.listdir(\"../input/cityofla/CityofLA/Additional data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will find and print all the files inside **Additional data** that has csv format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "csvfiles=[]\n",
    "for file in additional:\n",
    "    if file.endswith('.csv'):\n",
    "        print(file)\n",
    "        csvfiles.append(\"../input/cityofla/CityofLA/Additional data/\"+file)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 3 comma seperated files inside the folder,\n",
    "1. job titles : contains the title given to different jobs available.\n",
    "2. sample job class export template.csv : contains sample job bulletin to csv export details.\n",
    "3. kaggle_data_dictionary : contains name and description of each column that is in sample job class export template."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the required csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_title=pd.read_csv(csvfiles[0])\n",
    "sample_job=pd.read_csv(csvfiles[1])\n",
    "kaggle_data=pd.read_csv(csvfiles[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting  basic ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the below section we will take a look at the three csv files which was just loaded to get basic understanding of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_title.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The are %d rows and %d cols in job_title file\" %(job_title.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    sample_job[sample_job['Field Name']=='SCHOOL_TYPE']['Description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The are %d rows and %d cols in sample_job file\" %(sample_job.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The are %d rows and %d cols in kaggle_data file\" %(kaggle_data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"There are %d text files in bulletin directory\" %len(bulletins))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting the headings from job bulletins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_headings(bulletin):       \n",
    "    \n",
    "    \"\"\"\"function to get the headings from text file\n",
    "        takes a single argument\n",
    "        1.takes single argument list of bulletin files\"\"\"\n",
    "    \n",
    "    with open(\"../input/cityofla/CityofLA/Job Bulletins/\"+bulletins[bulletin]) as f:    ##reading text files \n",
    "        data=f.read().replace('\\t','').split('\\n')\n",
    "        data=[head for head in data if head.isupper()]\n",
    "        return data\n",
    "        \n",
    "def clean_text(bulletin):      \n",
    "    \n",
    "    \n",
    "    \"\"\"function to do basic data cleaning\n",
    "        takes a single argument\n",
    "        1.takes single argument list of bulletin files\"\"\"\n",
    "                                            \n",
    "    \n",
    "    with open(\"../input/cityofla/CityofLA/Job Bulletins/\"+bulletins[bulletin]) as f:\n",
    "        data=f.read().replace('\\t','').replace('\\n','')\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Headings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will print headings from first two job bulletins file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_headings(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_headings(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we can observe that there is some specefic pattern or format which is kept while writing job bulletins.\n",
    "- The order of the headings almost coincides with each other,which will be beneficial for our task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def to_dataframe(num,df):\n",
    "    \"\"\"\"function to extract features from job bulletin text files and convert to\n",
    "    pandas dataframe.\n",
    "    function take two arguments \n",
    "                        1.the number of files to be read\n",
    "                        2.dataframe object                                      \"\"\"\n",
    "    \n",
    "\n",
    "    \n",
    "    opendate=re.compile(r'(Open [D,d]ate:)(\\s+)(\\d\\d-\\d\\d-\\d\\d)')       #match open date\n",
    "    \n",
    "    salary=re.compile(r'\\$(\\d+,\\d+)((\\s(to|and)\\s)(\\$\\d+,\\d+))?')       #match salary\n",
    "    \n",
    "    requirements=re.compile(r'(REQUIREMENTS?/\\s?MINIMUM QUALIFICATIONS?)(.*)(PROCESS NOTE)')      #match requirements\n",
    "    \n",
    "    for no in range(0,num):\n",
    "        with open(\"../input/cityofla/CityofLA/Job Bulletins/\"+bulletins[no],encoding=\"ISO-8859-1\") as f:         #reading files \n",
    "                try:\n",
    "                    file=f.read().replace('\\t','')\n",
    "                    data=file.replace('\\n','')\n",
    "                    headings=[heading for heading in file.split('\\n') if heading.isupper()]             ##getting heading from job bulletin\n",
    "\n",
    "                    sal=re.search(salary,data)\n",
    "                    date=datetime.strptime(re.search(opendate,data).group(3),'%m-%d-%y')\n",
    "                    try:\n",
    "                        req=re.search(requirements,data).group(2)\n",
    "                    except Exception as e:\n",
    "                        req=re.search('(.*)NOTES?',re.findall(r'(REQUIREMENTS?)(.*)(NOTES?)',\n",
    "                                                              data)[0][1][:1200]).group(1)\n",
    "                    \n",
    "                    duties=re.search(r'(DUTIES)(.*)(REQ[A-Z])',data).group(2)\n",
    "                    try:\n",
    "                        enddate=re.search(\n",
    "                                r'(JANUARY|FEBRUARY|MARCH|APRIL|MAY|JUNE|JULY|AUGUST|SEPTEMBER|OCTOBER|NOVEMBER|DECEMBER)\\s(\\d{1,2},\\s\\d{4})'\n",
    "                                ,data).group()\n",
    "                    except Exception as e:\n",
    "                        enddate=np.nan\n",
    "                    \n",
    "                    selection= [z[0] for z in re.findall('([A-Z][a-z]+)((\\s\\.\\s)+)',data)]     ##match selection criteria\n",
    "                    \n",
    "                    df=df.append({'File Name':bulletins[no],'Position':headings[0].lower(),'salary_start':sal.group(1),\n",
    "                               'salary_end':sal.group(5),\"opendate\":date,\"requirements\":req,'duties':duties,\n",
    "                                'deadline':enddate,'selection':selection},ignore_index=True)\n",
    "                    \n",
    "                    \n",
    "                    reg=re.compile(r'(One|Two|Three|Four|Five|Six|Seven|Eight|Nine|Ten|one|two|three|four)\\s(years?)\\s(of\\sfull(-|\\s)time)')\n",
    "                    df['EXPERIENCE_LENGTH']=df['requirements'].apply(lambda x :  re.search(reg,x).group(1) if re.search(reg,x) is not None  else np.nan)\n",
    "                    df['FULL_TIME_PART_TIME']=df['EXPERIENCE_LENGTH'].apply(lambda x:  'FULL_TIME' if x is not np.nan else np.nan )\n",
    "                    \n",
    "                    reg=re.compile(r'(One|Two|Three|Four|Five|Six|Seven|Eight|Nine|Ten|one|two|three|four)(\\s|-)(years?)\\s(college)')\n",
    "                    df['EDUCATION_YEARS']=df['requirements'].apply(lambda x :  re.search(reg,x).group(1) if re.search(reg,x) is not None  else np.nan)\n",
    "                    df['SCHOOL_TYPE']=df['EDUCATION_YEARS'].apply(lambda x : 'College or University' if x is not np.nan else np.nan)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print('umatched sequence')\n",
    "                    \n",
    "                    \n",
    "                \n",
    "                \n",
    "        \n",
    "           \n",
    "    return df\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "df=pd.DataFrame(columns=['File Name','Position','salary_start','salary_end','opendate','requirements','duties','deadline'])\n",
    "df=to_dataframe(len(bulletins),df)\n",
    "df.to_csv('job class output.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the data dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dictionary=pd.DataFrame({'Field Name':['File Name','Position','salary_start','salary_end','opendate',\n",
    "                                            'requirements','duties','deadline','selection','EXPERIENCE_LENGTH','FULL_TIME_PART_TIME','EDUCATION_YEARS','SCHOOL_TYPE'],\n",
    "                             })\n",
    "\n",
    "data_dictionary['Description']=['The file name of the job bulletin from which each record came','The title of the particular class (e.g., Systems Analyst, Carpenter)',\n",
    "                              'The overall salary start','The overall maximum salary','The date the job bulletin opened','Overall requirement that has to be filled',\n",
    "                              'A summary of what someone does in the particular job\\n','The date the job bulletin closed','list of selection criterias','Years required in a particular job class or external role.',\n",
    "                              'Whether the required experience is full-time, part','Years required in a particular education program',\n",
    "                               'School Type: School type required (e.g. college or university, high school)']\n",
    "\n",
    "data_dictionary['Data Type']=['string']*13\n",
    "\n",
    "data_dictionary['Accepts Null Values?']=['Yes']*13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dictionary.to_csv('data dictionary.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now we have a proper comma seperataed file containing most of the information we need.we will now start exploring it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here,\n",
    "- **Position** implies JOB_CLASS_TITLE\n",
    "- **Duties** implies JOB_DUTIES\n",
    "- **Requirements** : All of the job requirements posted under requirements section\n",
    "- **Salary** has been broken down to **Salary_start** and **Salary_end** which signifies the salary range.\n",
    "    for example $ 5000 to $ 6000 \n",
    "        salary_start = 5000\n",
    "        salary_end = 6000\n",
    "- **Opendate** : The job application open date\n",
    "- **deadline** : deadline for applying for the job.\n",
    "- **selection** : The list of  selection criterias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## which are the common job sectors in LA?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There are %d different jobs available' %df['Position'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "text=''.join(job for job in df['Position'])                                ##joining  data to form text\n",
    "text=word_tokenize(text)\n",
    "jobs=Counter(text)                                                         ##counting number of occurences\n",
    "jobs_class=[job for job in jobs.most_common(12) if len(job[0])>3]          ##selecting most common words\n",
    "#offers=[job[1] for job in jobs.most_common(12) if len(job[0]>3)]\n",
    "a,b=map(list, zip(*jobs_class))\n",
    "sns.barplot(b,a,palette='rocket')                                           ##creating barplot\n",
    "plt.title('Job sectors')\n",
    "plt.xlabel(\"count\")\n",
    "plt.ylabel('sector')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can see that **service sector** dominates in creating opputunities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It might be a good idea to have **different display boards** for service and product sectors,so that candidates can easily find job of their preference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\n",
    "    convert salary to proper  form \n",
    "    by removing '$' and ',' symbols.\n",
    "                                    \"\"\"\n",
    "\n",
    "df['salary_start']=[int(sal.split(',')[0]+sal.split(',')[1] ) for sal in df['salary_start']]   \n",
    "df['salary_end']=[sal.replace('$','')  if sal!= None else 0 for sal in df['salary_end']  ]\n",
    "df['salary_end']=[int(sal.split(',')[0]+sal.split(',')[1] ) if type(sal)!=int else 0 for sal in df['salary_end']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What about the salary distribution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,5))\n",
    "sns.distplot(df['salary_start'])\n",
    "plt.title('salary distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we can observe that thet salaries typically varies from $50k to $150k.\n",
    "- Most jobs salary start from $80000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which are the best paid jobs in LA?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "'''finding the most paid 10 jobs at LA'''\n",
    "\n",
    "most_paid=df[['Position','salary_start']].sort_values(by='salary_start',ascending=False)[:10]\n",
    "plt.figure(figsize=(7,5))\n",
    "sns.barplot(y=most_paid['Position'],x=most_paid['salary_start'],palette='rocket')\n",
    "plt.title('Best paid jobs in LA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which the jobs with highest salary deviation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''''calculating salary start - salary end '''\n",
    "\n",
    "df['salary_diff']=abs(df['salary_start']-df['salary_end'])\n",
    "\n",
    "ranges=df[['Position','salary_diff']].sort_values(by='salary_diff',ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,5))\n",
    "sns.barplot(y=ranges['Position'],x=ranges['salary_diff'],palette='RdBu')   ##plotting\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Has job opportunities really increased recently?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "'''Extracting year out of opendate timestamp object and counting\n",
    "    the number of each occurence of each year using count_values() '''\n",
    "\n",
    "df['year_of_open']=[date.year for date in df['opendate']]\n",
    "\n",
    "count=df['year_of_open'].value_counts(ascending=True)\n",
    "years=['2020','2019','2018', '2017', '2016', '2015', '2014', '2013', '2012', '2008', '2006',\n",
    "           '2005', '2002', '1999']\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.plot([z for z in reversed(years)],count.values,color='blue')\n",
    "\n",
    "plt.title('Oppurtunities over years')\n",
    "plt.xlabel('years')\n",
    "plt.ylabel('count')\n",
    "plt.gca().set_xticklabels([z for z in reversed(years)],rotation='45')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It is evident from the above graph that job oppurtunities is constantly increasing after 2012 or so. \n",
    "- job oppurtunities has never decreased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What about the full time experience?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experience=df['EXPERIENCE_LENGTH'].value_counts().reset_index()\n",
    "experience['index']=experience['index'].apply(lambda x : x.lower())\n",
    "experience=experience.groupby('index',as_index=False).agg('sum')\n",
    "labels=experience['index']\n",
    "sizes=experience['EXPERIENCE_LENGTH']\n",
    "plt.figure(figsize=(5,7))\n",
    "plt.pie(sizes,explode=(0, 0.1, 0, 0,0,0,0),labels=labels)\n",
    "plt.gca().axis('equal')\n",
    "plt.title('Experience value count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I think it is clear that 50 percent of the job requires atleat two years experience in the field.\n",
    "- This has a negative effect of keeping freshers away from entering the job sector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experience or College degree, which is more preferred ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1=df['SCHOOL_TYPE'].value_counts()[0]\n",
    "x2=df['FULL_TIME_PART_TIME'].value_counts()[0]\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.bar(height=[x1,x2],x=['College Degree','Experience'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Almost all jobs bulletins specify the experience needed by candidate to qualify for the job.\n",
    "  But only some post the required educational baground.This might be in the assumption that an experienced candidate will surely posses College or university degree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which month of the year offers most opportunities?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Extracting month out of opendate timestamp object and counting\n",
    "    the number of each occurence of each months using count_values() '''\n",
    "\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "df['open_month']=[z.month for z in df['opendate']]\n",
    "count=df['open_month'].value_counts(sort=False)\n",
    "sns.barplot(y=count.values,x=count.index,palette='rocket')\n",
    "plt.gca().set_xticklabels([calendar.month_name[x] for x in count.index],rotation='45')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there is more job opportunities created in the months of **March,October and December**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* - It  might be a good idea to declare these months as **hiring months** and this could even attract a lot of potential candidates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which day of the week ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Extracting weekday out of opendate timestamp object and counting\n",
    "    the number of each occurence of each weekday using count_values() '''\n",
    "\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "\n",
    "df['open_day']=[z.weekday() for z in df['opendate']]\n",
    "count=df['open_day'].value_counts(sort=False)\n",
    "sns.barplot(y=count.values,x=count.index,palette='rocket')\n",
    "plt.gca().set_xticklabels([calendar.day_name[x] for x in count.index],rotation='45')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow ! All the postings are open from **friday** ! This is pretty interesting.\n",
    "- Is there any specific reason behind this?\n",
    "We will try to find out.\n",
    "anyway its good to post all opening in a day  of week so  that candidates can refer the bulletins and apply without missing any openings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### what about deadlines ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('%d job applications may close without prior notice' %df['deadline'].isna().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I think employers should post a deadline for job application in the job bulletins to avoid any unwanted confusions between candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['dealine']=df['deadline'].fillna(method='backfill',inplace=True\n",
    "#deadline=[datetime.strptime(x,'%B %d, %Y')  for x in df['deadline'] ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "req=' '.join(text for text in df['requirements'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Cloud of Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def show_wordcloud(data, title = None):\n",
    "    \n",
    "    \n",
    "    '''funtion to produce and display wordcloud\n",
    "        taken 2 arguments\n",
    "        1.data to produce wordcloud\n",
    "        2.title of wordcloud'''\n",
    "    \n",
    "    \n",
    "    wordcloud = WordCloud(\n",
    "        background_color='white',\n",
    "        stopwords=set(STOPWORDS),\n",
    "        max_words=250,\n",
    "        max_font_size=40, \n",
    "        scale=3,\n",
    "        random_state=1 # chosen at random by flipping a coin; it was heads\n",
    "    ).generate(str(data))\n",
    "\n",
    "    fig = plt.figure(1, figsize=(12, 12))\n",
    "    plt.axis('off')\n",
    "    if title: \n",
    "        fig.suptitle(title, fontsize=20)\n",
    "        fig.subplots_adjust(top=2.3)\n",
    "\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.show()\n",
    "show_wordcloud(text,'REQUIREMENTS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most influential words in requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lem=WordNetLemmatizer()\n",
    "text=[lem.lemmatize(w) for w in word_tokenize(req)]\n",
    "vect=TfidfVectorizer(ngram_range=(1,3),max_features=100)\n",
    "vectorized_data=vect.fit_transform(text)\n",
    "#id_map=dict((v,k) for k,v in vect.vocabulary_.items())\n",
    "vect.vocabulary_.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word 2 Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_corpus(df,col):\n",
    "    \n",
    "    '''function to build corpus from dataframe'''\n",
    "    lem=WordNetLemmatizer()\n",
    "    corpus= []\n",
    "    for x in df[col]:\n",
    "        \n",
    "        \n",
    "        words=word_tokenize(x)\n",
    "        corpus.append([lem.lemmatize(w) for w in words])\n",
    "    return corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=build_corpus(df,'requirements')\n",
    "model = word2vec.Word2Vec(corpus, size=100, window=20, min_count=30, workers=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsne_plot(model,title='None'):\n",
    "    \"Creates and TSNE model and plots it\"\n",
    "    labels = []\n",
    "    tokens = []\n",
    "\n",
    "    for word in model.wv.vocab:\n",
    "        tokens.append(model[word])\n",
    "        labels.append(word)\n",
    "    \n",
    "    tsne_model = TSNE(perplexity=80, n_components=2, init='pca', n_iter=2500, random_state=23)\n",
    "    new_values = tsne_model.fit_transform(tokens)\n",
    "\n",
    "    x = []\n",
    "    y = []\n",
    "    for value in new_values:\n",
    "        x.append(value[0])\n",
    "        y.append(value[1])\n",
    "        \n",
    "    plt.figure(figsize=(12, 12)) \n",
    "    plt.title(title)\n",
    "    for i in range(len(x)):\n",
    "        plt.scatter(x[i],y[i])\n",
    "        plt.annotate(labels[i],\n",
    "                     xy=(x[i], y[i]),\n",
    "                     xytext=(5, 2),\n",
    "                     textcoords='offset points',\n",
    "                     ha='right',\n",
    "                     va='bottom')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_plot(model,'Requirements')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we can see clusters of words used in **Requirement** section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are the common requirements for any post?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token=word_tokenize(req)\n",
    "counter=Counter(token)\n",
    "count=[x[0] for x in counter.most_common(40) if len(x[0])>3]\n",
    "print(\"Most common words in Requirement\")\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It can be observed that companies prefer  \n",
    "- **experienced** \n",
    "- **educated professionals**  having **degree from an accredicted university**\n",
    "- also willing to work **full-time**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To improve the diversity employers are encourged to give chance to **Freshers** and people willing to work **part time**.\n",
    "     This can attract greater pool of applicants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Duties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duties= ' '.join(d for d in df['duties'])\n",
    "show_wordcloud(duties,'Duties')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most influential and common words in duties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lem=WordNetLemmatizer()\n",
    "text=[lem.lemmatize(w) for w in word_tokenize(duties)]\n",
    "vect=TfidfVectorizer(ngram_range=(1,3),max_features=200)\n",
    "vectorized_data=vect.fit_transform(text)\n",
    "#id_map=dict((v,k) for k,v in vect.vocabulary_.items())\n",
    "vect.vocabulary_.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token=word_tokenize(duties)\n",
    "counter=Counter(token)\n",
    "count=[x[0] for x in counter.most_common(40) if len(x[0])>3]\n",
    "print(\"Most common words in Duties\")\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word 2 Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=build_corpus(df,'duties')\n",
    "model = word2vec.Word2Vec(corpus, size=100, window=20, min_count=40, workers=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_plot(model,'Duties')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Latent Dirichlet Allocation (LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic modeling is a type of statistical modeling for discovering the abstract “topics” that occur in a collection of documents. Latent Dirichlet Allocation (LDA) is an example of topic model and is used to classify text in a document to a particular topic. It builds a topic per document model and words per topic model, modeled as Dirichlet distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lem=WordNetLemmatizer()\n",
    "text=[lem.lemmatize(w) for w in word_tokenize(duties)]\n",
    "vect=TfidfVectorizer(ngram_range=(1,3),max_features=200)\n",
    "vectorized_data=vect.fit_transform(text)\n",
    "id2word=dict((v,k) for k,v in vect.vocabulary_.items())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=gensim.matutils.Sparse2Corpus(vectorized_data,documents_columns=False)\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus,id2word=id2word,num_topics=8,random_state=34,passes=25,per_word_topics=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamodel.show_topic(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  What is the Dominant topic and its percentage contribution in each document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In LDA models, each document is composed of multiple topics. But, typically only one of the topics is dominant. The below code extracts this dominant topic for each sentence and shows the weight of the topic and the keywords in a nicely formatted output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "def format_topics_sentences(ldamodel, corpus, texts):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row_list in enumerate(ldamodel[corpus]):\n",
    "        row = row_list[0] if ldamodel.per_word_topics else row_list            \n",
    "        # print(row)\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "\n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=ldamodel, corpus=corpus, texts=build_corpus(df,'duties'))\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "df_dominant_topic.dropna(inplace=True)\n",
    "df_dominant_topic.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic modeling visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=build_corpus(df,'duties')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Counts of Topic Keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When it comes to the keywords in the topics, the importance (weights) of the keywords matters. Along with that, how frequently the words have appeared in the documents is also interesting to look.\n",
    "\n",
    "Let’s plot the word counts and the weights of each keyword in the same chart.\n",
    "\n",
    "You want to keep an eye out on the words that occur in multiple topics and the ones whose relative frequency is more than the weight. Often such words turn out to be less important. The chart I’ve drawn below is a result of adding several such words to the stop words list in the beginning and re-running the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "topics = ldamodel.show_topics(formatted=False)\n",
    "data_flat = [w for w_list in build_corpus(df,'duties') for w in w_list]\n",
    "counter = Counter(data_flat)\n",
    "\n",
    "out = []\n",
    "for i, topic in topics:\n",
    "    for word, weight in topic:\n",
    "        out.append([word, i , weight, counter[word]])\n",
    "\n",
    "df_plot= pd.DataFrame(out, columns=['word', 'topic_id', 'importance', 'word_count'])        \n",
    "\n",
    "# Plot Word Count and Weights of Topic Keywords\n",
    "fig, axes = plt.subplots(4, 2, figsize=(10,12), sharey=True, dpi=160)\n",
    "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    ax.bar(x='word', height=\"word_count\", data=df_plot.loc[df_plot.topic_id==i, :], color=cols[i], width=0.5, alpha=0.3, label='Word Count')\n",
    "    ax_twin = ax.twinx()\n",
    "    ax_twin.bar(x='word', height=\"importance\", data=df_plot.loc[df_plot.topic_id==i, :], color=cols[i], width=0.2, label='Weights')\n",
    "    ax.set_ylabel('Word Count', color=cols[i])\n",
    "    #ax_twin.set_ylim(0, 0.040); ax.set_ylim(0, 4000)\n",
    "    ax.set_title('Topic: ' + str(i), color=cols[i])\n",
    "    ax.tick_params(axis='y', left=False)\n",
    "    ax.set_xticklabels(df_plot.loc[df_plot.topic_id==i, 'word'], rotation=30, horizontalalignment= 'right')\n",
    "    ax.legend(loc='upper left'); ax_twin.legend(loc='upper right')\n",
    "\n",
    "fig.tight_layout(w_pad=2)    \n",
    "fig.suptitle('Word Count and Importance of Topic Keywords',y=1)    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are the most common selection criterias?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(7,7))\n",
    "count=df['selection'].astype(str).value_counts()[:10]\n",
    "sns.barplot(y=count.index,x=count,palette='rocket')\n",
    "plt.gca().set_yticklabels(count.index,rotation='45')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It is evident that  **interview** ,**Essay** and **Questionnaire** are the most common selection criterias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is there any Gender bias in job bulletins?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the follow section i am trying investigate if there is any gender biased terms used in **Requirement** and **Duties** section of the job bulletin.   \n",
    "For that i will pos tag all the text data in the requirement field and then,\n",
    "- Extract the words having pronoun tag.\n",
    "- check if any gender biased terms like he/she is used in the field.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pronoun(data):\n",
    "    \n",
    "    '''function to tokenize data and perform pos_tagging.Returns tokens having \"PRP\" tag'''\n",
    "    \n",
    "    prn=[]\n",
    "    vrb=[]\n",
    "    token=word_tokenize(data)\n",
    "    pos=pos_tag(token)\n",
    "   \n",
    "    vrb=Counter([x[0] for x in pos if x[1]=='PRP'])\n",
    "    \n",
    "    return vrb\n",
    "    \n",
    "\n",
    "\n",
    "req_prn=pronoun(req)\n",
    "duties_prn=pronoun(duties)\n",
    "print('pronouns used in requirement section are')\n",
    "print(req_prn.keys())\n",
    "print('\\npronouns used in duties section are')\n",
    "print(duties_prn.keys())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Surprisingly, i couldn't find any gender biased or racist pronouns in **Requirement ** or **Duties section**\n",
    "2. you can see all the pronouns used are neutral."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Is there any gender bias in job titles?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this section we will try and find out if there is any gender bias in job titles.\n",
    "    A **gender-specific job title** is a name of a job that also specifies or implies the gender of the person performing that job. \n",
    "    the job title **policeman** implies that the person is male. A gender-neutral job title, on the other hand, is one that does not specify or imply       gender, such as firefighter or a lawyer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names=['senior waterman','policeman']\n",
    "for name in names:\n",
    "    z=re.match(r'\\w+?\\s?\\w+(man|women)$',name)\n",
    "    print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- So you can see that the code works well.We will try in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in df['Position']:\n",
    "    z=re.match(r'\\w+?\\s?\\w+(man|women)$',name)\n",
    "    if z is not None:\n",
    "        print(z)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Nothing ! I think the authorities hava done a good job by removing possible gender biases from job titles and renamed it suitably."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding similar jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here in this section I am trying to find similar jobs given a job title.\n",
    "- **Assumption** : I assume that similar jobs have similar job title.    \n",
    "               \n",
    "I have used jaccard distance to find the text similarity and compare it with a threashold value to obtain the jobs.     \n",
    "This method can output the jobs the same domain so that candidates who posses the right skills can apply for similar jobs in the domain.I believe clustering jobs can improve the pool of applicants to great extend.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def similar_jobs(job):\n",
    "    \n",
    "    ''' function to find and return jobs with similar job title.take a single argument\n",
    "            - job title\n",
    "            returns\n",
    "                -list of similar jobs '''\n",
    "    \n",
    "    word1=word_tokenize(job)\n",
    "    jobs=[]\n",
    "    for i,name in enumerate(df['Position']):\n",
    "        word2=word_tokenize(name)\n",
    "        distance=jaccard_distance(set(ngrams(word1,n=1)),set(ngrams(word2,n=1)))\n",
    "        if(distance<.55):\n",
    "            jobs.append((name,i))\n",
    "    return jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_jobs(df['Position'][10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- you can clearly observe that the jobs outputted are from the same domain and are moreover similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similar_req(job):\n",
    "    \n",
    "    ''' function to find and return jobs with similar job title.take a single argument\n",
    "            - job title\n",
    "            returns\n",
    "                -list of similar jobs '''\n",
    "    \n",
    "    word1=word_tokenize(job)\n",
    "    jobs=[]\n",
    "    for i,name in enumerate(df['requirements']):\n",
    "        word2=word_tokenize(name)\n",
    "        distance=jaccard_distance(set(ngrams(word1,n=1)),set(ngrams(word2,n=1)))\n",
    "        if(distance<.5):\n",
    "            jobs.append((name,df.iloc[i]['Position']))\n",
    "    return jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_req(df['requirements'][10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['requirements'][312]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Readability Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- FleschKincaid Grade Level       \n",
    "Designed to indicate how difficult a reading passage is to understand. The result is a number that corresponds with a U.S grade level.\n",
    "> FKGL = 0.39 * (total words/ total \n",
    "sentences) + 11.8 (total syllables/ \n",
    "total words) -15.59\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Flesch Index  -------         Text file reading Grade**\n",
    "\n",
    "   0-30      ---------              College                 \n",
    "\n",
    "   50-60     ---------               High School                          \n",
    "\t\n",
    "   90-100    ---------                Fourth Grade\n",
    "\n",
    "\t\n",
    "1. From above the flesch-kincaid Grade level formula is used to compute the equivalent Grade level G −"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reading=[]\n",
    "for file in df['File Name']:\n",
    "    text=open(\"../input/cityofla/CityofLA/Job Bulletins/\"+file,'r',encoding=\"ISO-8859-1\").read()\n",
    "    sentence = text.count('.') + text.count('!') + text.count(';') + text.count(':') + text.count('?')\n",
    "    words = len(text.split())\n",
    "    syllable = 0\n",
    "    for word in text.split():\n",
    "        for vowel in ['a','e','i','o','u']:\n",
    "            syllable += word.count(vowel)\n",
    "        for ending in ['es','ed','e']:\n",
    "            if word.endswith(ending):\n",
    "                   syllable -= 1\n",
    "        if word.endswith('le'):\n",
    "            syllable += 1\n",
    "            \n",
    "    G = round((0.39*words)/sentence+ (11.8*syllable)/words-15.59)\n",
    "    reading.append(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Plotting Grade level distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(reading)\n",
    "plt.xlabel('Flesch Index')\n",
    "plt.title('Flesch index distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we can see that index is always between 0-30,which comes under college grade.May be city of LA officials should look in to this and moderate their bulletins to make it simple to read !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##          More to come,stay tuned !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you like my kernel please consider upvoting.Thank you :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
